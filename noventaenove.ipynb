{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "#Bibliotecas\n",
    "\n",
    "import os, email, nltk, numpy\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "def tokenize_email(fullpath):\n",
    "    \n",
    "    # esta função recebe um caminho de arquivo de email e extrai um vetor de palavras\n",
    "    # ao extrair o vetor de palavras, estas recebem alguns tratamentos, descritos a seguir\n",
    "    \n",
    "    try:\n",
    "        # abre o arquivo de email e lê os dados do arquivo\n",
    "        open_email = open(fullpath, \"r\")\n",
    "        read_email = open_email.read()\n",
    "        # extrai a mensagem do email, e depois o corpo do email\n",
    "        data_email = email.message_from_string(read_email) \n",
    "        message_email = data_email.get_payload() \n",
    "        # cria um reconhecedor de padrões e os extrai, criando um vetor de elementos reconhecidos\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "        tokens_email = tokenizer.tokenize(message_email)\n",
    "        # coloca todas as letras em minúsculas\n",
    "        tolower_email = [word.lower() for word in tokens_email]\n",
    "        # substitui todos os números encontrados pela string 'numeros'\n",
    "        for i, token in enumerate(tolower_email):\n",
    "            if token.isnumeric():\n",
    "                tolower_email[i] = 'numeros'\n",
    "        # remove as 'stopwords'\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        stopwords.append('_______________________________________________')\n",
    "        stopped_email = [word for word in tolower_email if word not in stopwords]\n",
    "        # retorna o email tokenizado\n",
    "        return(stopped_email)\n",
    "        \n",
    "    except:\n",
    "        # caso ocorra algum erro, retorna FALSO\n",
    "        return(0)\n",
    "\n",
    "def tokenize_directory(directory):\n",
    "    \n",
    "    # essa função aplica a função tokenize_email a todos os arquivos de um diretório dado\n",
    "\n",
    "    # inicializando a lista de emails tokenizados\n",
    "    lista = []\n",
    "\n",
    "    #iterando sobre os arquivos do diretório\n",
    "    for file in os.listdir(directory):\n",
    "    \n",
    "        # extraindo e passando o parâmetro para a função tokenize_email\n",
    "        filename = os.fsdecode(file)\n",
    "        fullpath = os.path.join(directory, filename)\n",
    "        email_tokens = tokenize_email(fullpath)\n",
    "        \n",
    "        # checa se a tokenização obteve sucesso, e no caso positivo adiciona o email tokenizado à lista\n",
    "        if email_tokens:\n",
    "            lista.append(email_tokens)\n",
    "    \n",
    "    #retorna a lista de emails tokenizados\n",
    "    return(lista)\n",
    "\n",
    "def email_vectorize(lista_emails, palavras_spam):\n",
    "    \n",
    "    sz1 = len(lista_emails)\n",
    "    sz2 = len(palavras_spam)\n",
    "    data = numpy.zeros((sz1, sz2), int)\n",
    "\n",
    "    for i, email in enumerate(lista_emails):\n",
    "        vector = numpy.zeros(sz2) \n",
    "        for j, item in enumerate(palavras_spam):\n",
    "            if item in email:\n",
    "                data[i][j] = 1\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "def listar_palavras_spam(spam_lista, size):\n",
    "    \n",
    "    palavras_spam = []    \n",
    "    tudo = [item for sublist in spam_lista for item in sublist]\n",
    "    for element in nltk.FreqDist(tudo).most_common(size):\n",
    "        palavras_spam.append(element[0])\n",
    "    \n",
    "    return(palavras_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = (\"/Users/andrevrpires/Downloads/ML/DadosSpam/spam/\")\n",
    "spam_lista = tokenize_directory(directory)\n",
    "\n",
    "directory = (\"/Users/andrevrpires/Downloads/ML/DadosSpam/nospameasy/\")\n",
    "nospam_lista = tokenize_directory(directory)\n",
    "\n",
    "directory = (\"/Users/andrevrpires/Downloads/ML/DadosSpam/hardham/\")\n",
    "hardnospam_lista = tokenize_directory(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_spam = listar_palavras_spam(spam_lista, 1000)\n",
    "\n",
    "data_spam = email_vectorize(spam_lista, palavras_spam)\n",
    "data_nospam = email_vectorize(nospam_lista, palavras_spam)\n",
    "data_hardnospam = email_vectorize(hardnospam_lista, palavras_spam)\n",
    "\n",
    "X = numpy.concatenate((data_spam, data_nospam, data_hardnospam), axis = 0)\n",
    "y = numpy.concatenate((numpy.zeros(data_spam.shape[0], int),\n",
    "                       numpy.ones(data_nospam.shape[0], int),\n",
    "                       numpy.ones(data_hardnospam.shape[0], int)), axis = 0)\n",
    "\n",
    "X, y = shuffle(X, y, random_state = 123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9924242424242424"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = LogisticRegression(solver = 'lbfgs').fit(X_train, y_train)\n",
    "model1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9919191919191919"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = svm.LinearSVC(max_iter = 3000).fit(X_train, y_train)\n",
    "model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9904040404040404"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = svm.SVC(gamma='scale').fit(X_train, y_train)\n",
    "model3.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
